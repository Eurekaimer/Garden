# 点估计

上一章讨论的是几个统计量的常用抽样分布以及充分统计量，还有因式分解定理用以判断是否为充分统计量，但是让我们回想一下引入充分统计量的目的


## 引言

讲述了一些参数估计的方法和脉络，大致如下


> [!tip] Parameter estimation
> + 点估计
> 	+ 无偏估计 UE
> 	+ 一致最小方差无偏估计 UMVUE
> 	+ 极大似然估计 MLE
> + 区间估计
> 	+ 枢轴量法
> 	+ Fisher信仰推断方法


>上一章中主要讲述了常用统计量的抽样分布及充分统计量，引进统计量的目的在于为了对于感兴趣问题进行统计推断(充分性原则)，实际中感兴趣的问题则是多与分布族的未知参数有关.
>那么接下来讨论的就应当是参数的估计和检验问题，本章讲述的就是参数的估计


Remark: 这里的参数指下面三类未知参数 
+ 分布中所含的未知参数
+ 分布中所含的未知参数的函数 $X\sim N(\mu,\sigma^{2}),P(X\le a)=\Phi \left( \frac{a-\mu}{\sigma} \right)$
+ 分布的各种特征数 $EX,Var(X)$


一般常用$\theta$来表示参数，参数$\theta$所有可能取值组成的集合称为**参数空间**，常用$\Theta$表示.**参数估计问题就是根据样本构造适当的统计量对上述各种未知参数做出估计.**


参数估计的形式有：点估计和区间估计


## 点估计的概念与无偏性

评价一个估计好坏的准则主要有三个

1. 无偏性
2. 有效性
3. 相合性(大样本性质)

### 点估计及无偏性

点估计的定义比较简单且显然就是用来估计未知参数的统计量，而构造统计量应当满足一定的合理性，最常见的合理性要求是无偏性


> [!NOTE] 无偏估计的定义
> 设$\hat{\theta}=\hat{\theta}\left( x_{1},\dots,x_{n} \right)$是$\theta$的一个估计，若是对于属于参数空间中任意的一个$\theta\in\Theta,$有$E_{\theta}\left( \hat{\theta} \right)=\theta$，则称$\hat{\theta}$是$\theta$的无偏估计，否则称为有偏估计


Remark: 完全可以将这个概念提前放置，然后就可以对前面的三大分布，样本方差的动机有比较好的初步了解(点名表扬zyd)

无偏估计没有系统偏差,也就是将所有误差加起来求平均的值为0



> [!Example] 一些例子
> 样本均值是总体均值的无偏估计，总体$k$阶矩存在时，样本$k$阶原点矩就是总体的无偏估计，但中心矩则不同，例如样本方差就不是总体方差的无偏估计而是渐进无偏估计因为在样本量趋于无穷时可近似看作$\sigma^{2}$的无偏估计


那么为了修正原本的样本方差$s_{n}^{2}$，将系数减一得到新的样本方差$s^{2}$，在小样本的场合更多使用$s^{2}$估计$\sigma^{2}$

大偏差通常被视为估计的不足，刀切法就是一种缩小偏差的方法


> [!info] 刀切法,jackknife(Quenouile,named after Tukey)
> 设$T(x)$是基于样本$x=\left( x_{1},\dots,x_{n} \right)$的关于参数$g(\theta)$的估计量，且满足关系$E_{\theta}\left( T(x) \right)=g(\theta)+O\left( \frac{1}{n} \right)$.以$x_{(-i)}$表示从样本中删去$x_{i}$后的向量，定义刀切统计量为
> $$
> T_{j}(x)=nT(x)- \frac{n-1}{n}\sum\limits_{i=1}^{n} T(x_{(-i)})
$$
可以证明这种定义的刀切统计量$E_{\theta}\left( T_{j}(x) \right)=g(\theta)+O\left( \frac{1}{n^{2}} \right)$，并且其方差不会增大



> [!help] Remark
> 关于刀切法的详细介绍可以参考Efron的专著



并不是所有的参数都存在无偏估计，存在时称为可估的，否则称为不可估



### 有效性

人们常用无偏估计的方差大小来衡量无偏估计优劣的标准，这就是有效性

一个无偏估计比另一个更有效的定义就是该无偏估计的方差小于等于另一个并且至少有一个$\theta$使得不等号严格成立


## 矩估计及相合性


### 替换原理和矩法估计

1900年$K·Pearson$提出的替换原理后来被称为矩法
+ 用样本矩替换总体矩
+ 用样本矩函数替换相应的总体矩函数

实质是用经验分布函数替换总体，依据Glivenko定理

### 概率函数已知时未知参数的矩估计

做法通常是反表示然后将已知的总体矩参量改为样本矩，然后做近似估计


### 相合性

设$\theta\in\Theta$为未知参数，给出$\hat{\theta_{n}}$是$\theta$的一个估计量，$n$是样本容量，如果对于任何正数$\epsilon$有
$$
\lim\limits_{ n \to \infty } P\left( \lvert \hat{\theta_{n}}-\theta \rvert\ge \epsilon  \right) =0
$$
那么称$\hat{\theta_{n}}$为参数的相合估计

相合性是对估计的一个最基本要求，也就是随着样本量的无限增大，精度应当也无限逼近实际，能够达到任意指定的精度，不满足相合性要求的估计不予考虑.

实质上也就是该估计量序列依概率收敛可应用依概率收敛的性质及各种大数定律

给出两个判断相合性的定理：


> [!TIP] TH6.2.1
> 设$\hat{\theta}_{n}=\hat{\theta}_{_{n}}(x_{1},\dots,x_{n})$是$\theta$的一个估计量，若
> $$
> \lim\limits_{ n \to \infty } E\left( \hat{\theta}_{n} \right) =\theta, \lim\limits_{ n \to \infty } Var(\hat{\theta}_{n})=0
>$$
>则$\hat{\theta}_{n}$是$\theta$的相合估计

`Proof.`

利用Chebyshev不等式把期望转换成方差，将$\lvert \hat{\theta}_{n}-E(\hat{\theta}_{n}) \rvert< \frac{\epsilon}{2}\implies \lvert \hat{\theta}_{n}-E(\hat{\theta}_{n}) \rvert< \frac{\epsilon}{2}$修改为概率不等式的形式即可

> [!TIP] TH6.2.2
> 若$\hat{\theta}_{n_{1}},\dots,\hat{\theta}_{n_{k}}$分别是$\theta_{1},\dots,\theta _k$的相合估计，$\eta=g\left( \theta_{1},\dots,\theta_{k} \right)$是$\theta_{1},\dots,\theta_{k}$的连续函数，则$\hat{\eta}_{n}=g\left( \hat{\theta}_{n_{1}},\dots,\hat{\theta}_{n_{k}} \right)$是$\eta$的相合估计


接下来就可以开始介绍一些实用的统计量

## 最大似然估计与EM算法

最大似然估计(MLE)最早由Gauss在1821针对正态分布提出，由R·A·Fisher再次提出并证明了它的一些性质使之得到了广泛的应用.

>本节给出最大似然估计的定义与计算和在某些复杂情况下MLE的一种有效算法——EM算法，并介绍最大似然估计的渐进正态性


### 最大似然估计

## 矩估计

矩估计也称为moment estimation是K.Pearson提出的，主要思想也就是用各种样本矩替代总体矩，而样本矩因为完全不依赖总体参数，是统计量而总体矩与参数有关，但是又因为大数定律和中心极限定理可知样本矩是总体矩的良好估计也就是说我们完全可以用样本矩估计总体矩而估计出未知参数.


主要有几个基本问题：
+ 使用样本矩估计总体矩，原点矩和中心矩都是矩估计
+ 估计方程组的存在唯一性取决于实际
+ 估计时尽量使用低阶矩


## 极大似然估计与EM算法

极大似然估计是Gauss提出、由Fisher推广的，本节除了给出极大似然估计的基本定义外还将给出求取某些复杂情况MLE的有效算法，即EM算法


### 极大似然估计


基本思想：概率最大的事件最有可能发生

满足这个不等式：选取的估计$\hat{p(x)}$
$$
P\left\{ X=x|\hat{p(x)} \right\} \geq P\left\{ X=x|p' \right\},\forall p',\forall x. 
$$

#### 似然函数


> [!NOTE] 似然函数(likehood function)
> 对于给定的样本观测值$x=\left( x_{1},x_{2},\dots,x_{n} \right)$，称$f(x,\theta)$为$\theta$的似然函数，简称为似然函数，记为$L(\theta,x)=f(x,\theta),\forall\theta\in\Theta$


极大似然估计也就是选取$\hat{\theta}$后似然函数取到最大值，实际求某个参数的MLE也就是求似然函数的极值问题，如果$L(\theta,x)$关于$\theta$可微，则$\theta$的MLE可以通过求偏导得到.



> [!NOTE] 似然方程
> 求解办法
> $$ \frac{\partial L(\theta,x)}{\partial\theta_{j}}=0,j=1,2\dots,k $$


一个很重要的例子就是对于正态总体，均值与方差的估计和MLE一样

#### MLE的不变性(Invariance of MLE)

Reference:
+ Statistic Inference (Casella) Theorem 7.2.10
+ [Wikipedia-MLE](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Functional_invariance)
+ [知乎一篇文章](https://www.zhihu.com/question/615795405)
+ 数理统计教程 王兆军 邹长亮

 ![关键图片](https://cdn.jsdelivr.net/gh/Eurekaimer/MyIMGs@main/img/%E5%B9%BF%E4%B9%89MLE%E5%AE%9A%E4%B9%89.png)


这个定理的内容本身十分直观，如下：


> [!NOTE] MLE的不变性
> 若$\hat{\theta}$是$\theta$的MLE，则对于$\theta$的任何函数$\tau(\theta)$,$\tau(\hat{\theta})$是$\tau(\theta)$的MLE.

`Proof.`

下面是参考的Casella的定义(这样就十分直观了)：

> [!tip] 诱导似然函数(Induced Likelihood Function)
> 如果原本的函数是一一对应的那么这种MLE的不变性实际上没有什么用处，所以我们比较关心的是非一对一情形，这也就提出了更一般的定理(任何函数的条件)，为了完成定理的证明首先应当做到将MLE的定义推广到函数上也就是我们研究的$\tau(\theta)$上.
> 为了达成这个目的我们应当先定义诱导似然函数，由下式给出：
> $$L^{*}(\eta|x)=\prod_{i=1}^{n} f(x_{i}|\tau^{-1}(\eta))=L(\tau^{-1}(\eta)|x)$$
>
> 并且有$\sup\limits_{\eta}L^{*}(\eta|x)=\sup\limits_{\eta}L(\tau^{-1}(\eta)|x)=\sup\limits_{\theta}L(\theta|x)$

具体的证明也比较简单只需要按照定义书写一点数学分析废话(吐槽一下NKU的课本没有写清楚的什么是$g(\theta)$的MLE,以至于笔者白白花费时间在错误的定义上百思不得其解,最终是自己查询了Casella和其他社区才得到答案)：
$$
\begin{aligned}
G(w)=\left\{ \theta:g(\theta)=w \right\} ,&M(w,x)=\sup\limits_{{\theta\in G(w)}}L(\theta|x)\\
M(g(\hat{\theta});x)&=\sup\limits_{\theta\in G(g(\hat{\theta}))}L(\theta|x)=L(\hat{\theta}|x)=\sup\limits_{\theta\in \Theta}L(\theta|x)\\
&\geqslant \sup\limits_{\theta\in G(w)}L(\theta|x)=M(w,x)
\end{aligned}
$$
注1：在这种定义下所谓的$M(w,x)$就摇身一变成为了$g(\theta)$的似然函数了，否则在原本的情况下根本无法讨论.
注2：建议想更深入一点还是去看Casella，前置门槛也不高(NKU教材省略过多在作者眼里可能不值一提的细节，导致虽然本身内涵丰富但是阅读的难度被无端拉高)

### MLE的相合性



> [!NOTE] Fisher信息量
> Contents


### EM算法






## 一致最小方差无偏估计(UMVUE)

### 均方误差(MSE)

$MSE(\hat{\theta})=E(\hat{\theta}-\theta)^{2}=Var(\hat{\theta})+(E \hat{\theta}-\theta)^{2}$

### 一致最小方差无偏估计(UMVUE)

设$\hat{\theta}$是参数$\theta$的无偏估计，若对于任意其它无偏估计$\overset{\sim}{\theta}$，都有$Var(\hat{\theta})\leqslant Var(\overset{\sim}{\theta})$




### UMVUE判断条件



### Cramer-Rao不等式(C-R Inequality)



### 完备统计量



> [!NOTE] Lehmann-Scheffe Theorem
> $X_{1},X_{2},\dots,X_{n}\overset{IID}{\sim}\left\{ f(x,\theta) ,\theta\in \Theta\right\}$，$T(X)$为$\theta$的充分完备统计量，若$\theta$可估，且$S(X)$是$\theta$的无偏估计，则$S_{0}(T)=E(S(X)|T)$是$\theta$的唯一UMVUE

`Proof.`

利用充分统计量构造两个新的UE再相减得到0的一个UE，然后使用Rao-Blackwell定理可以得到放缩式

## Bayes估计

### 





### MCMC(Markov Chain Monte Carlo Method)






### Bootstrap



## 习题


> [!tldr] HW3
> 王兆军(2023) 数理统计教程(第二版)
> + 习题二 3、15、25
> 
> 茆诗松(2010) 概率论与数理统计(第二版)
> + 习题6.1 3、8、10、11
> + 习题6.2 3、4、6
> 
> 额外习题


额外习题如下：

设$\left\{f(x,\theta);\theta\in\Theta\right\}$是某参数分布族，如果

$$f(x,\theta)=c(\theta)\exp\left(\sum_{i=1}^kc_i(\theta)T_i(x)\right)h(x),$$

则称此分布族为指数型分布族，其中$k$ 为正整数，$c(\theta)>0,h(x)>0.$ 

一个PDF$f(x,\theta)$ 的支撑集定义为：$S=\{x:f(x,\theta)>0\}.$

证明：

(1)指数型分布族具有共同支撑集；

(2)含两个未知参数的 Weibull 分布$W(\alpha,\theta)$不是指数型分布族，

而$\Gamma$分布族、$\beta$分布族、Poisson分布族都是指数型分布族。




> [!tldr] HW4
> 王兆军(2023) 数理统计教程(第二版)
> + 习题二 3、15、25
> 
> 额外习题




> [!tldr] HW5
> 一道编程实现补充题如下


> [!NOTE] 补充题
> 设样本$X_1,\cdots,X_n \sim Gamma(\alpha,\lambda)$,其密度函数为$f(x,\alpha,\lambda) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}$.取参数 $\alpha$ 和 $\lambda$ 的值分别为$\alpha_0$ 和 $\lambda_0$.产生随机样本 $X_1,\cdots,X_n$。
> (a) 用 Newton-Raphson 算法和 Fisher scoring 算法估计这两个参数
> (b) 随机产生30组初值，分别用这两个算法得到结果，并比较这两个算法。
> (c) 设 $\alpha$ 已知，Newton-Raphson 算法和 Fisher scoring 算法估计参数 $\lambda$。并与结果 (a) 相比较。
> 涉及程序的内容，编程实现，并把程序和结果打印出来。程序可以字体变小后打印。

(a)

已知Gamma分布的密度函数为$f(x,\alpha,\lambda) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}$,可以计算对数似然函数的值为
$$
l(\alpha,\lambda|x)=n\alpha \ln\lambda+(\alpha-1)\sum\limits_{i=1}^{n} \ln X_{i}-\lambda \sum\limits_{i=1}^{n} X_{i}-n\ln\Gamma(\alpha)
$$
Newton-Raphson算法：$\theta_{K+1}=\theta_{k}-H(\theta_{k})^{-1}g(\theta_{k})$

设参数向量$(\theta = (\alpha,\lambda)^T)$，迭代公式为$\theta_{k + 1}=\theta_{k}-H(\theta_{k})^{-1}g(\theta_{k})$，其中$g(\theta)=(\frac{\partial l}{\partial\alpha},\frac{\partial l}{\partial\lambda})^T$ 是梯度向量，$H(\theta)$是Hesse矩阵:
$$
g(\theta)=\begin{pmatrix}\frac{\partial l}{\partial\alpha}\\\frac{\partial l}{\partial\lambda }\end{pmatrix}=\begin{pmatrix} nlnλ−n\frac{Γ(α)}{Γ′(α)}​+\sum\limits_{i=1}^{n} ​lnX_{i}​\\ \frac{nα}{\lambda}​−\sum\limits_{i=1}^{n} ​X_{i}​\end{pmatrix}
$$

$$
H(\theta)=\begin{pmatrix}\frac{\partial^{2}l}{\partial\alpha^{2}}&\frac{\partial^{2}l}{\partial\alpha\partial\lambda}\\\frac{\partial^{2}l}{\partial\lambda\partial\alpha}&\frac{\partial^{2}l}{\partial\lambda^{2}}\end{pmatrix}=\begin{pmatrix}
-n \frac{\Gamma''(\alpha)\Gamma(\alpha)-(\Gamma'(\alpha)^{2})}{\Gamma^{2}(\alpha)}&\frac{n}{\lambda}\\\frac{n}{\lambda}&-\frac{n\alpha}{\lambda^{2}}
\end{pmatrix}
$$

Fisher scoring算法：$\theta_{K+1}=\theta_{k}+I(\theta_{k})^{-1}g(\theta_{k})$

其中$I(\theta)=-E\left[ H(\theta) \right]$，用样本估计的观测Fisher信息矩阵代替期望Fisher信息矩阵(矩阵中无与$X_{i}$有关的量，求期望不变)

(b)


> [!NOTE] 随堂测试
> 例3.4.9(韦来生)设$X=(X_1,\cdots,X_n)$为从 Poisson 分布$P(\lambda)$中抽取的简单随机样本，求$(1)g_1(\lambda)=\lambda;(2)g_2(\lambda)=\lambda^r,~r>0$为自然数；$(3)~g_3(\lambda)=P_\lambda(X_1=x)$的$UMVUE.$

`Sol.`

由 2.7 节和 2.8 节可知$T=T(\boldsymbol{X})=\sum_i=1^nX_i$为充分完全统计量

(1)令$h_1(T)=T/n$, $E(h_1(T))=E(\overline{X})=\lambda$,故$h_1(T)$是充分完全统计量$T$的函数，且是$\lambda$的无偏估计，故由 L-S 定理可知$h_1(T)$是$\lambda$的UMVUE

(2)由于$T\sim P(n\lambda)$,令$h_2(T)$为$g_2(\lambda)=\lambda^r$的无偏估计，故有$E_\lambda[h_2(T)]=g_2(\lambda)$,即

$$\sum_{t=0}^\infty h_2(t)\frac{e^{-n\lambda}(n\lambda)^t}{t!}=\lambda^r.$$

此式等价于



$$\sum_{t=0}^\infty h_2(t)\frac{n^t\lambda^t}{t!}=\lambda^re^{n\lambda}.$$



将上式右边作展开得

$$\lambda^re^{n\lambda}=\sum_{l=0}^\infty\frac{n^l\lambda^{l+r}}{l!}=\sum_{t=r}^\infty\frac{n^{t-r}\lambda^t}{(t-r)!}.$$

将其代入前一式右边得

$$\sum_{t=0}^\infty h_2(t)\frac{n^t\lambda^t}{t!}=\sum_{t=r}^\infty\frac{n^{t-r}\lambda^t}{(t-r)!}.$$

上述等式两边是$\lambda$的幂级数，比较其系数得

$$\begin{aligned}&h_{2}(t)=0,\quad\text{当 }t=0,1,\cdots,r-1,\\&h_{2}(t)=\frac{t!\:n^{t-r}}{(t-r)!n^t}=\frac{t(t-1)\cdots(t-r+1)}{n^r},\quad\text{当 }t=r,r+1,\cdots.\end{aligned}$$

综合上述两式得

$$h_2(T)=\frac{T(T-1)\cdots(T-r+1)}{n^r},$$

为$g_2(\lambda)=\lambda^r$的无偏估计，$h_2(T)$是充分完全统计量$T$的函数，故由 L-S 定理可知$h_2(T)$为$g_2(\lambda)$的 UMVUE

(3)由$P_\lambda(X_1=x)=e^{-\lambda}\lambda^x/x!$,可见它是参数$\lambda$的函数，故可用$g_3(\lambda)$表示令$\varphi(X_1)=I_[X_1=x]$,则$E_\lambda[\varphi(X_1)]=P_\lambda(X_1=x).$因此$\varphi(X_1)$为$g_3(\lambda)$的无偏估计. 注意到$T=T(\boldsymbol X)=\sum_i=1^nX_i\sim P(n\lambda)$和$\sum_i=2^nX_i\sim P((n-1)\lambda)$,故有

$$h_3(t)=E(\varphi(X_1)|T=t)=P_\lambda(X_1=x|T=t)=\frac{P_\lambda(X_1=x,T=t)}{P_\lambda(T=t)}$$
$$=\frac{P_\lambda(X_1=x)P_\lambda(X_2+\cdots+X_n=t-x)}{P_\lambda(X_1+\cdots+X_n=t)}=\frac{(n-1)^{t-x}t!}{n^t(t-x)!x!}$$
$$=\binom{t}{x}\frac{(n-1)^{t-x}}{n^t}=\binom{t}{x}\Big(\frac{1}{n}\Big)^x\Big(1-\frac{1}{n}\Big)^{t-x},\quad t\geqslant x.$$

由引理 3.4.1 可知$h_3(T)$为$g_3(\lambda)$的无偏估计，它又是充分完全统计量$T$的函数

所以由 L-S 定理可知

$$h_3(T)=\begin{pmatrix}T\\x\end{pmatrix}\left(\frac1n\right)^x\left(1-\frac1n\right)^{T-x}$$

为$g_3(\lambda)$的 UMVUE



