# 基本概念


> [!tldr] 重点
> 1. 三大分布 ⭐⭐⭐⭐⭐
> 2. 抽样定理 ⭐⭐⭐⭐⭐
> 3. 充分统计量 ⭐⭐⭐⭐
> 4. 因子分解定理 ⭐⭐⭐⭐


## 总体与样本


### 总体与个体


抛开实际背景，用一个概率分布去描述和归纳总体也是合理的，那么可以说总体就是一个分布，以后从总体中抽样与从某分布中抽样是一个意思．


### 样本

样本的所谓二重性：
+ 样本是随机变量，在抽样前不可知
+ 观测后就是确定的值，可以用数列表示

为方便起见，样本无论是样本本身还是所谓观测值都是用$x_{1},x_{2},\dots,x_{n}$表示

简单随机抽样方法可以得到简单随机样本，这种方法需要满足一定的基本假设：也就是随机性和独立性，还需要排除人为的干扰.显然的，在样本量很大的条件下应当认为不放回的抽样方式也满足独立性.


## 样本数据的整理与显示


### 经验分布函数

关于经验分布函数的重要性，一般可以通过它对于分布函数有良好的逼近性质阐述，而且这种逼近性质中的点点收敛可以通过SLLN证明，另一个一致收敛性则是另一个定理的内容：

下面就是这个很重要的定理，但是大多数数理统计书没有给出严格的证明，可是它的重要性在Wikipedia中的一段话可见一斑(甚至有时叫做统计学基本定理)：


>In the [theory of probability](https://en.wikipedia.org/wiki/Theory_of_probability "Theory of probability"), the **Glivenko–Cantelli theorem** (sometimes referred to as the **Fundamental Theorem of Statistics**), named after [Valery Ivanovich Glivenko](https://en.wikipedia.org/wiki/Valery_Ivanovich_Glivenko "Valery Ivanovich Glivenko") and [Francesco Paolo Cantelli](https://en.wikipedia.org/wiki/Francesco_Paolo_Cantelli "Francesco Paolo Cantelli"), describes the asymptotic behaviour of the [empirical distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function "Empirical distribution function") as the number of [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables "Independent and identically distributed random variables") observations grows. Specifically, the empirical distribution function [converges uniformly](https://en.wikipedia.org/wiki/Uniform_convergence "Uniform convergence") to the true distribution function [almost surely](https://en.wikipedia.org/wiki/Almost_surely "Almost surely").



> [!NOTE] Glivenko-Cantelli theorem
> $X_{1},X_{2},\dots X_{n} \overset{IID}{\sim}F(x)\implies F_{n}(x)\overset{a.s.}{\to} F(x)(SL LN)$, and this result is used Strong Law of Large Numbers, however, this theorem(Glivenko-Cantelli Theorem) strength this result by proving uniform convergence(a.s.) of $F_{n}\to F$ 

`Proof.`

假设是连续的情形，然后假设选取$n$个样本点，并且满足$-\infty=x_{0}\leq x_{1}\leq x_{2}\dots \leq x_{n}=\infty$

这种假设会给出$F_{n}(x_{j+1})-F_{n}(x_{j})=\frac{1}{n}$，并且给出一对放缩式$x\in \left[ x_{j-1},x_{j} \right]$

$$
\begin{align}
F_{n}(x)-F(x)&\leq F_{n}(x_{j})-F(x_{j-1})=F_{n}(x_{j-1})-F(x_{j-1})+\frac{1}{n}\\
F_{n}(x)-F(x)&\geq F_{n}(x_{j-1})-F(x_{j})=F_{n}(x_{j})-F(x_{j})-\frac{1}{n}
\end{align}
$$

那么我们将得到$\forall x,\lVert F_{n}(x)-F(x) \rVert=\sup\limits_{{x\in R}}\lvert F_{n}(x)-F(x) \rvert\leq \max\limits_{j=\left\{ 1,2,\dots,n \right\}}\lvert F_{n}(x_{j})-F(x_{j}) \rvert+\frac{1}{n}\implies Q.E.D$


### 频数频率分布表

### 样本数据的图形显示

+ 直方图
+ 茎叶图

我们在小学二年级学过这些内容了


## 统计量及其分布


### 统计量与抽样分布

主要是两个基本概念

若是样本函数$T$中不含有任何未知参数，称为统计量，统计量的分布称为抽样分布.

Remark：统计量和抽样分布的概念在后面的内容中都有涉及

以下就是几个重要统计量和它们的延申概念的介绍

### 样本均值及其抽样分布

很容易给出样本均值的定义(算术平均值)：$\overline{x}= \frac{\sum\limits_{i=1}^{n}x_{i}}{n}$就是样本均值

并且它还具有以下性质(Proof is trivial)
1. $\sum\limits_{i=1}^{n}(x_{i}-\overline{x})=0$
2. $\forall c, \sum\limits(x_{i}-c)^{2}\geq \sum\limits(x_{i}-\overline{x})^{2}$
3. $\overline{x}\sim N\left( \mu, \frac{\sigma^{2}}{n} \right)$


### 样本方差与样本标准差


样本方差(**无偏方差**):$s^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}(x_{i}-\overline{x})^{2}$

而且我们还可以说明它为什么是无偏的也就是$E(s^{2})=\sigma^{2}$

`Proof.`

$$
E\left( \sum\limits_{i=1}^{n} (x_{i}-\overline{x})^{2} \right)=nE(x_{i}^{2})-nE(\overline{x}^{2})=n(\mu^{2}+\sigma^{2})-n\left( \mu^{2}+\frac{\sigma^{2}}{n} \right) =(n-1)\sigma^{2}
$$

若是有偏的也是一种估计，并且叫做极大似然估计(MLE)


### 样本矩及其函数

常见统计量除了样本均值与样本方差外的更一般推广是样本矩：
+ $k$阶原点矩：$a_{k}=\frac{1}{n}\sum\limits_{i=1}^{n}x_{i}^{k}$
+ $k$阶中心矩：$b_{k}=\frac{1}{n}\sum\limits_{i=1}^{n}\left( x_{i}-\overline{x} \right)^{k}$

关于样本中心距还有样本偏度和样本峰度两个重要的指标，也是中心距函数：
+ 样本偏度：$\hat{\beta_{s}}=\frac{b_{3}}{b_{2}^{\frac{3}{2}}}$ 
	+ 反映总体分布密度曲线与对称性的偏离方向和程度
+ 样本峰度：$\hat{\beta_{k}}=\frac{b_{4}}{b_{2}^{2}}-3$ 
	+ 反映总体分布密度曲线在其峰值附近的陡峭程度和尾部粗细


### 次序统计量及其分布

简单来说按照大小顺序将样本排序并编号就可以称作次序统计量

我们关心次序统计量的分布情况，为简单起见先讨论单个次序统计量的分布如下：

其中第$k$个次序统计量$x_{(k)}$的密度函数为($p(x)$为总体$X$的密度函数)

$$
p_{k}(x)=\frac{n!}{(k-1)!(n-k)!}\left( F(x) \right)^{k-1}\left( 1-F(x) \right) ^{n-k}p(x) 
$$

很容易理解前面的系数计算的是置换数，然后根据类似指标函数的概率的做法得到后面的$F$和$1-F$，再加上取该点的概率值

然后再讨论多个次序统计量的分布：下面讨论的是**任意两个**次序统计量的分布，其余同理

$$
p_{ij}(x)=\frac{n!}{(i-1)!(j-i-1)!(n-j)!}\left[ F(y) \right]^{i-1}\left[ F(z)-F(y) \right] ^{j-i-1}\left[ 1-F(z) \right] ^{n-j}p(y)p(z) 
$$

同样可以按照一元的情形类似讨论即可

实际问题中也会用到一些有关次序统计量的函数$R_{n}=x_{(n)}-x_{(1)}$称为样本极差


### 样本分位数与样本中位数

样本中位数是比较常见的统计量并且表示起来也简单，我们将直接给出更一般的$p$分位数$m_{p}$的定义（包括不加证明的给出$n\to \infty$时的渐进分布）


中位数

$$
\begin{align}
m_{0.5}=\left\{ \begin{matrix}
x_{\left( \frac{n+1}{2} \right) }&,2\nmid n \\
\frac{1}{2}\left( x_{\left( \frac{n}{2} \right)}+x_{\left( \frac{n}{2}+1 \right)} \right) &,2\mid n
\end{matrix} \right. 
\end{align}
$$

$p$分位数

$$
\begin{align}
m_{p}=\left\{ \begin{matrix}
x_{\left( \lceil np+1  \rceil  \right) }&,np\not\in \mathbb{Z} \\
\frac{1}{2}\left( x_{\left( np  \right) }+x_{\left( np+1  \right) } \right) &,np\in \mathbb{Z}
\end{matrix} \right. 
\end{align}
$$

渐进分布的一个定理：
总体密度函数为$p(x)$，$x_{p}$为$p$分位数，且有连续性和非负性，那么$n\to \infty$的样本$p$分位数为$m_{p}$的渐进分布为：

$$
m_{p}\sim N\left( x_{p}, \frac{p(1-p)}{n\cdot p^{2}(x_{p})} \right),m_{0.5}\sim N\left( x_{p}, \frac{1}{4n\cdot p^{2}(x_{0.5})} \right)
$$

中位数相比样本均值有更好的对极端值的抗干扰性，也叫做稳健性

以下是次序统计量的应用部分：五数概括与箱线图(Box-Whisker plot)

### 五数概括与箱线图

所谓五数概括就是指最小观测值，最大观测值，第一、三四分位数，中位数

利用这五个数可以大致表现样本数据分布的形状，箱线图大致分为
+ 左偏分布
+ 右偏分布
+ 对称分布


## 三大抽样分布

统计学的三大分布都是利用正态分布导出的，充分体现了正态分布的重要性，三大分布的导出方式也需要记住和掌握，还需要多利用三大分布完成习题.

下面再额外给出三大分布有关的工具(分布族)

>统计分布常用于总体的建模，因此我们处理的往往不是单个的分布，而是一族分布。**一个分布族共用一个函数形式，其中包含一个或多个参数，用以确定具体的分布。**


### $\Gamma$分布族

$X\sim \Gamma(\alpha,\beta)$






### $\chi^{2}$分布(卡方分布)

`K·Pearson`

$\chi^{2}$的构造为$\chi^{2}=\sum\limits_{i=1}^{n}x_{i}^{2}$，那么如何得到它的概率密度函数(PDF)?

利用正态分布的性质可以得到，若是$X\sim N(0,1),\chi^{2}\sim Ga\left( \frac{1}{2}, \frac{1}{2} \right)$，根据可加性得到卡方分布的概率密度函数为$p(y)=\chi^{2}(n)=Ga\left( \frac{n}{2}, \frac{1}{2} \right)= \frac{\left( \frac{1}{2} \right)^{\frac{n}{2}}}{\Gamma\left( \frac{n}{2} \right)}y^{\frac{n}{2}-1}e^{- \frac{y}{2}}$


一般在各个教材中都会提到卡方分布的一个重要应用是下面的一个定理

设$x_{1},x_{2},\dots,x_{n}$是来自正态总体$N(\mu,\sigma^{2})$的样本，其样本均值与样本方差分别为

$$
\overline{x}= \frac{1}{n}\sum\limits_{i=1}^{n} x_{i},s^{2}= \frac{1}{n-1}\sum\limits_{i=1}^{n} \left( x_{i}-\overline{x} \right) ^{2}
$$
则有

(1)$\overline{x}$与$s^{2}$相互独立
(2)$\overline{x}\sim N\left( \mu, \frac{\sigma^{2}}{n} \right)$
(3)$\dfrac{(n-1)s^{2}}{\sigma^{2}}\sim \chi^{2}(n-1)$

### $F$分布

构造方法是$X_{1}\sim \chi^{2}(m),X_{2}\sim \chi^{2}(n)$，并且独立，使得$F= \frac{\frac{X_{1}}{m}}{\frac{X_{2}}{n}}\sim F(m,n)$ 

也可以看出$F=\frac{\frac{\sum\limits_{i=1}^{m}y_{i}^{2}}{m}}{\frac{\sum\limits_{i=1}^{n}x_{i}^{2}}{n}}$,并根据一系列变换得到$\frac{\Gamma \left( \frac{m+n}{2} \right)\left( \frac{m}{n} \right)^{\frac{m}{2}}}{\Gamma \left( \frac{m}{2}\Gamma \left( \frac{n}{2} \right) \right)}y^{\frac{m}{2}-1}\left( 1+ \frac{m}{n}y \right)^{- \frac{m+n}{2}}$

利用$\chi^{2}$分布的性质可以得到相关$F$分布的性质：$X_{i}\sim N(\mu_{1},\sigma_{1}^{^{2}}),Y_{i}\sim N(\mu_{2},\sigma_{2}^{^{2}})$

并且两个样本相互独立的情况下，有$F= \dfrac{s_{x}^{2}/\sigma_{1}^{2}}{s_{y}^{2}/\sigma_{2}^{2}}\sim F(m-1,n-1)$




### $t$分布

`Gosset and R·A·Fisher`

$t= \frac{y_{1}}{\sqrt{ \left( \sum\limits_{i=1}^{n}x_{i}^{2} \right) /n }}$,若是两个随机变量独立，$X_{1}\sim N(0,1),X_{2}\sim \chi^{2}(n),t=\frac{X_{1}}{\sqrt{ X_{2} / n }}$


$X_{1},-X_{1}\overset{IID}{\sim}N(0,1)$，那么$t^{2}= \frac{X_{1}^{2}}{X_{2}^{2} / n}\sim F(1,n)$,也称为自由度为$n$的$t$分布


要想证明它与标准正态分布有良好的近似性质，可以考虑使用[Slutsky定理]()避免讨论它的概率密度函数而直接导出，关于Slutsky定理的证明可能还需要[Continuous mapping theorem](https://en.wikipedia.org/wiki/Continuous_mapping_theorem) ，有关这个Continuous mapping theorem的一部分证明还需要用到[Portmanteau theorem](https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures)(也是weak convergence的等价性质)


`Proof`


因为$\frac{X_{2}}{n}\sim E[X_{i}^{2}]=1$，而且分子趋向于标准正态分布，再使用$Slutsky$定理，可以直接得到$t$分布依概率收敛到标准正态分布


或者也可以直接使用PDF进行证明，$p_{t}(y)= \frac{\Gamma \left( \frac{n+1}{2} \right)}{\sqrt{ n\pi }\Gamma \left( \frac{n}{2} \right)}\left( 1+ \frac{y^{2}}{n} \right)^{-\frac{n+1}{2}}$



## 充分统计量


### 充分性的概念

>"样本加工不损失信息"称为充分性

一个简单的定义：在给定统计量$T$的取值后，$x_{1},\dots,x_{n}$的条件分布与$\theta$无关，则称统计量$T$是$\theta$的充分统计量

也就是$F_{\theta}(X|T=t)$不依赖于参数$\theta$，此条件分布已不含$\theta$的信息

### 因子分解定理


> [!TIP] 充分性原则
> 在充分统计量存在的场合，任何统计推断都可以基于充分统计量进行，这样可以简化统计推断的程序，通常将该原则称为充分性原则


引入概率函数也就是将PDF和PMF结合统一起来的概念，给出Neyman的因子分解定理(相比定义可以更加简单的判断一个统计量是否充分)



> [!NOTE] 因子分解定理(Neyman)
> 总体概率函数$f(x,\theta),x_{1},\dots,x_{n}$是样本 ，那么$T$是充分统计量的充分必要条件是：$\exists g(t,\theta),h(x_{1},\dots,x_{n})$使得任意的$\theta$和任一组观测值$x_{1},\dots,x_{n}$,有：
> $$
> f(x_{1},\dots,x_{n};\theta)=g(T(x_{1},\dots,x_{n}),\theta)h(x_{1},\dots,x_{n})
>$$
>其中$g(t,\theta)$是通过统计量$T$的取值而依赖于样本


那么进行推广得到更一般的命题也就是：

若统计量$T$是充分统计量，存在某个函数$h(\cdot)$使得$T$可以表示为$t=h(s)$，则说明统计量$S$也是充分统计量





## 习题


> [!tldr] HW1
> 茆诗松(2010) 概率论与数理统计(第二版)
> + 习题5.1 1、3、4
> + 习题5.3 1、4、6、9、12、20、23

评价：总的来说这次作业还是比较水，里面出现了一些很无聊的用基础概念的描述题，其他也是很基础的用概率论基本知识就足以完成，值得注意的只有一道计算样本偏度和样本峰度的题目(估计不会考，计算量有点ex了)



> [!tldr] HW2
> 王兆军(2023) 数理统计教程(第二版)
> + 习题一 8、9、13、14、15、24、26、28、31、41

评价：总的习题质量比较高，需要对课本中的定理和分布有比较熟练的掌握，没有什么特别难的题

















